{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ddcb3d",
   "metadata": {},
   "source": [
    "# FILE IN PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f90dce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "from typing import Tuple, List, Optional\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ea1a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Dataset and preprocessing\n",
    "# ----------------------------\n",
    "class LFWPairsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads pairs from an LFW pairs file and returns (img1, img2, label)\n",
    "    label: 1 for same person, 0 for different\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_dir: str,\n",
    "        pairs_file: str,\n",
    "        image_size: Tuple[int, int] = (100, 100),\n",
    "        max_pairs: Optional[int] = None,\n",
    "        augment: bool = False,\n",
    "    ):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.pairs_file = pairs_file\n",
    "        self.image_size = image_size\n",
    "        self.max_pairs = max_pairs\n",
    "        self.augment = augment\n",
    "\n",
    "        self.pairs = self._parse_pairs_file(self.pairs_file)\n",
    "        if self.max_pairs is not None:\n",
    "            self.pairs = self.pairs[: self.max_pairs]\n",
    "\n",
    "    def _parse_pairs_file(self, path: str) -> List[Tuple[str, int, str, int, int]]:\n",
    "        pairs = []\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "        # Skip header if present (first line often an integer count)\n",
    "        start_idx = 0\n",
    "        if lines and lines[0].split()[0].isdigit():\n",
    "            start_idx = 1\n",
    "\n",
    "        for ln in lines[start_idx:]:\n",
    "            parts = ln.split()\n",
    "            # same-person format: name i j\n",
    "            # diff-person format: name1 i name2 j\n",
    "            if len(parts) == 3:\n",
    "                name = parts[0]\n",
    "                i = int(parts[1])\n",
    "                j = int(parts[2])\n",
    "                img1 = self._image_path(name, i)\n",
    "                img2 = self._image_path(name, j)\n",
    "                label = 1\n",
    "            elif len(parts) == 4:\n",
    "                name1 = parts[0]\n",
    "                i = int(parts[1])\n",
    "                name2 = parts[2]\n",
    "                j = int(parts[3])\n",
    "                img1 = self._image_path(name1, i)\n",
    "                img2 = self._image_path(name2, j)\n",
    "                label = 0\n",
    "            else:\n",
    "                continue\n",
    "            pairs.append((img1, img2, label))\n",
    "        return pairs\n",
    "\n",
    "    def _image_path(self, person_name: str, image_num: int) -> str:\n",
    "        fn = f\"{person_name}_{image_num:04d}.jpg\"\n",
    "        return os.path.join(self.dataset_dir, person_name, fn)\n",
    "\n",
    "    def _load_image(self, path: str) -> np.ndarray:\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = cv2.resize(img, self.image_size)\n",
    "        if self.augment:\n",
    "            # Lightweight augmentation\n",
    "            if random.random() < 0.5:\n",
    "                img = cv2.flip(img, 1)\n",
    "            if random.random() < 0.2:\n",
    "                alpha = 1.0 + (random.random() - 0.5) * 0.2\n",
    "                beta = (random.random() - 0.5) * 10\n",
    "                img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        # Standardize per-image lightly for CNN stability\n",
    "        mean = float(img.mean())\n",
    "        std = float(img.std())\n",
    "        if std < 1e-3:\n",
    "            std = 1e-3\n",
    "        img = (img - mean) / std\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        p1, p2, label = self.pairs[idx]\n",
    "        img1 = self._load_image(p1)\n",
    "        img2 = self._load_image(p2)\n",
    "        # Add channel dimension\n",
    "        img1 = np.expand_dims(img1, axis=0)\n",
    "        img2 = np.expand_dims(img2, axis=0)\n",
    "        return torch.from_numpy(img1), torch.from_numpy(img2), torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd63e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Model: Siamese CNN\n",
    "# ----------------------------\n",
    "class ConvEmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_dim: int = 128):\n",
    "        super().__init__()\n",
    "        # Input: 1 x 100 x 100\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),  # -> 16 x 100 x 100\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # -> 16 x 50 x 50\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # -> 32 x 50 x 50\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # -> 32 x 25 x 25\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # -> 64 x 25 x 25\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # -> 64 x 12 x 12 (floor)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 12 * 12, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.fc(x)\n",
    "        # Normalize embedding (helps contrastive training and cosine distance)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, embedding_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.backbone = ConvEmbeddingNet(embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward_once(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.forward_once(x1), self.forward_once(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fad7425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Loss: Contrastive Loss (Hadsell et al. 2006)\n",
    "# ----------------------------\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, emb1: torch.Tensor, emb2: torch.Tensor, label_same: torch.Tensor) -> torch.Tensor:\n",
    "        # label_same: 1 for same, 0 for different\n",
    "        # Distance in embedding space\n",
    "        dist = F.pairwise_distance(emb1, emb2, p=2)\n",
    "        pos_loss = label_same * dist.pow(2)\n",
    "        neg_loss = (1 - label_same) * F.relu(self.margin - dist).pow(2)\n",
    "        loss = 0.5 * (pos_loss + neg_loss)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36ae8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def compute_distances(model: SiameseNet, loader: DataLoader, device: torch.device) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    dists, labels = [], []\n",
    "    for x1, x2, y in loader:\n",
    "        x1 = x1.to(device)\n",
    "        x2 = x2.to(device)\n",
    "        e1, e2 = model(x1, x2)\n",
    "        dist = F.pairwise_distance(e1, e2, p=2)\n",
    "        dists.append(dist.cpu().numpy())\n",
    "        labels.append(y.numpy())\n",
    "    return np.concatenate(dists), np.concatenate(labels)\n",
    "\n",
    "\n",
    "def tune_threshold(dists: np.ndarray, labels: np.ndarray) -> float:\n",
    "    # Sweep thresholds over observed distances\n",
    "    mins, maxs = float(np.min(dists)), float(np.max(dists))\n",
    "    grid = np.linspace(mins, maxs, num=200)\n",
    "    best_thr, best_f1 = grid[0], -1.0\n",
    "    for thr in grid:\n",
    "        preds = (dists <= thr).astype(np.int32)  # same if distance <= thr\n",
    "        tp = int(((preds == 1) & (labels == 1)).sum())\n",
    "        fp = int(((preds == 1) & (labels == 0)).sum())\n",
    "        tn = int(((preds == 0) & (labels == 0)).sum())\n",
    "        fn = int(((preds == 0) & (labels == 1)).sum())\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thr = thr\n",
    "    return float(best_thr)\n",
    "\n",
    "\n",
    "def evaluate_metrics(dists: np.ndarray, labels: np.ndarray, threshold: float) -> Tuple[float, float, float, float, Tuple[int, int, int, int]]:\n",
    "    preds = (dists <= threshold).astype(np.int32)\n",
    "    tp = int(((preds == 1) & (labels == 1)).sum())\n",
    "    fp = int(((preds == 1) & (labels == 0)).sum())\n",
    "    tn = int(((preds == 0) & (labels == 0)).sum())\n",
    "    fn = int(((preds == 0) & (labels == 1)).sum())\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn + 1e-8)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    rec = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "    return acc, prec, rec, f1, (tp, fp, tn, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a44f19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "def train(\n",
    "    model: SiameseNet,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: Optional[DataLoader],\n",
    "    device: torch.device,\n",
    "    epochs: int = 5,\n",
    "    lr: float = 1e-3,\n",
    "    margin: float = 1.0,\n",
    "):\n",
    "    criterion = ContrastiveLoss(margin=margin)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "        for x1, x2, y in pbar:\n",
    "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "            e1, e2 = model(x1, x2)\n",
    "            loss = criterion(e1, e2, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{running / (pbar.n or 1):.4f}\")\n",
    "\n",
    "        if val_loader is not None:\n",
    "            dists, labels = compute_distances(model, val_loader, device)\n",
    "            thr = tune_threshold(dists, labels)\n",
    "            acc, prec, rec, f1, (tp, fp, tn, fn) = evaluate_metrics(dists, labels, thr)\n",
    "            print(f\"[Val] thr={thr:.3f} acc={acc:.3f} prec={prec:.3f} rec={rec:.3f} f1={f1:.3f} | TP={tp} FP={fp} TN={tn} FN={fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed0e890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset_dir DATASET_DIR]\n",
      "                             [--pairs_train PAIRS_TRAIN]\n",
      "                             [--pairs_val PAIRS_VAL] [--pairs_test PAIRS_TEST]\n",
      "                             [--image_size IMAGE_SIZE IMAGE_SIZE]\n",
      "                             [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--lr LR] [--margin MARGIN]\n",
      "                             [--embedding_dim EMBEDDING_DIM]\n",
      "                             [--max_pairs_train MAX_PAIRS_TRAIN]\n",
      "                             [--max_pairs_eval MAX_PAIRS_EVAL]\n",
      "                             [--num_workers NUM_WORKERS] [--seed SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/derek/Library/Jupyter/runtime/kernel-v30311aadbb2e8674fb667f8a7d0618fe375fb5b43.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derek/.pyenv/versions/3.12.11/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Siamese CNN Face Verification on LFW\")\n",
    "parser.add_argument(\"--dataset_dir\", type=str, default=\"FaceRecognitionDset/lfw_funneled\")\n",
    "parser.add_argument(\"--pairs_train\", type=str, default=\"FaceRecognitionDset/pairsDevTrain.txt\")\n",
    "parser.add_argument(\"--pairs_val\", type=str, default=\"FaceRecognitionDset/pairsDevTest.txt\")\n",
    "parser.add_argument(\"--pairs_test\", type=str, default=\"FaceRecognitionDset/pairs.txt\")\n",
    "parser.add_argument(\"--image_size\", type=int, nargs=2, default=[100, 100])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--epochs\", type=int, default=5)\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "parser.add_argument(\"--margin\", type=float, default=1.0)\n",
    "parser.add_argument(\"--embedding_dim\", type=int, default=128)\n",
    "parser.add_argument(\"--max_pairs_train\", type=int, default=800)\n",
    "parser.add_argument(\"--max_pairs_eval\", type=int, default=400)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=0)\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "args = parser.parse_args()\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# If running in IPython/Jupyter, force num_workers=0 to avoid multiprocessing pickling issues\n",
    "if \"IPython\" in sys.modules or \"ipykernel\" in sys.modules:\n",
    "    if args.num_workers != 0:\n",
    "        print(\"Detected notebook environment; setting num_workers=0 for DataLoader to avoid multiprocessing issues.\")\n",
    "        args.num_workers = 0\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Datasets\n",
    "train_ds = LFWPairsDataset(\n",
    "    dataset_dir=args.dataset_dir,\n",
    "    pairs_file=args.pairs_train,\n",
    "    image_size=tuple(args.image_size),\n",
    "    max_pairs=args.max_pairs_train,\n",
    "    augment=True,\n",
    ")\n",
    "val_ds = LFWPairsDataset(\n",
    "    dataset_dir=args.dataset_dir,\n",
    "    pairs_file=args.pairs_val,\n",
    "    image_size=tuple(args.image_size),\n",
    "    max_pairs=args.max_pairs_eval,\n",
    "    augment=False,\n",
    ")\n",
    "test_ds = LFWPairsDataset(\n",
    "    dataset_dir=args.dataset_dir,\n",
    "    pairs_file=args.pairs_test,\n",
    "    image_size=tuple(args.image_size),\n",
    "    max_pairs=args.max_pairs_eval,\n",
    "    augment=False,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "# Model\n",
    "model = SiameseNet(embedding_dim=args.embedding_dim).to(device)\n",
    "\n",
    "# Train\n",
    "train(model, train_loader, val_loader, device, epochs=args.epochs, lr=args.lr, margin=args.margin)\n",
    "\n",
    "# Tune threshold on validation\n",
    "dists_val, labels_val = compute_distances(model, val_loader, device)\n",
    "thr = tune_threshold(dists_val, labels_val)\n",
    "print(f\"Chosen threshold from validation: {thr:.3f}\")\n",
    "\n",
    "# Evaluate on test\n",
    "dists_test, labels_test = compute_distances(model, test_loader, device)\n",
    "acc, prec, rec, f1, (tp, fp, tn, fn) = evaluate_metrics(dists_test, labels_test, thr)\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {acc:.3f}, Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}\")\n",
    "print(f\"TP={tp} FP={fp} TN={tn} FN={fn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
